{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10, Train Loss: 0.6277038758027487, Val Loss: 0.6802504850758446\n",
      "Epoch 2/10, Train Loss: 0.43485903723279856, Val Loss: 0.9730860312779744\n",
      "Epoch 3/10, Train Loss: 0.2988508968150982, Val Loss: 0.9646177874671088\n",
      "Epoch 4/10, Train Loss: 0.20401031925088034, Val Loss: 1.2026039173205694\n",
      "Epoch 5/10, Train Loss: 0.13524196227211194, Val Loss: 1.299395184384452\n",
      "Epoch 6/10, Train Loss: 0.09014942599642585, Val Loss: 1.6374515374501546\n",
      "Epoch 7/10, Train Loss: 0.06087037356174941, Val Loss: 1.8656105001767476\n",
      "Epoch 8/10, Train Loss: 0.051734759807573666, Val Loss: 1.862462208006117\n",
      "Epoch 9/10, Train Loss: 0.035505242419738695, Val Loss: 2.101059687137604\n",
      "Epoch 10/10, Train Loss: 0.026297169066829003, Val Loss: 2.145445545514425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\王祖政\\AppData\\Local\\Temp\\ipykernel_32656\\1438812275.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(data, dtype=torch.long)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.2664728472692662, Val Loss: 0.18822716510537282\n",
      "Epoch 2/3, Train Loss: 0.11072215254480054, Val Loss: 0.23038433155294655\n",
      "Epoch 3/3, Train Loss: 0.052521710382633084, Val Loss: 0.2896714705044492\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    headlines = [item['headline'] for item in data]\n",
    "    labels = [item['is_sarcastic'] for item in data]\n",
    "    return headlines, labels\n",
    "\n",
    "# Preprocess data for LSTM\n",
    "def preprocess_data(headlines, labels, max_num_words=20000, max_sequence_length=30):\n",
    "    tokenizer = Tokenizer(num_words=max_num_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(headlines)\n",
    "    sequences = tokenizer.texts_to_sequences(headlines)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "    return np.array(padded_sequences), np.array(labels), tokenizer\n",
    "\n",
    "# LSTM Dataset class\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Build LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=64, output_dim=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        output = self.fc(hidden[-1])\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "# Train LSTM\n",
    "def train_lstm_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "# Fine-tune BERT\n",
    "def fine_tune_bert(train_texts, train_labels, val_texts, val_labels, max_length=30, batch_size=16, epochs=3):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    train_dataset = SarcasmDataset(train_encodings['input_ids'], train_labels)\n",
    "    val_dataset = SarcasmDataset(val_encodings['input_ids'], val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device, dtype=torch.long)\n",
    "            outputs = model(input_ids=data, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data, labels = data.to(device), labels.to(device, dtype=torch.long)\n",
    "                outputs = model(input_ids=data, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    file_path = \"Sarcasm_Headlines_Dataset.json\"\n",
    "    headlines, labels = load_data(file_path)\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(headlines, labels, test_size=0.2, random_state=42)\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Preprocess for LSTM\n",
    "    train_data, train_labels, tokenizer = preprocess_data(train_texts, train_labels)\n",
    "    val_data, val_labels, _ = preprocess_data(val_texts, val_labels)\n",
    "\n",
    "    train_dataset = SarcasmDataset(train_data, train_labels)\n",
    "    val_dataset = SarcasmDataset(val_data, val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    # Train LSTM\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    lstm_model = LSTMModel(vocab_size).to(device)\n",
    "    train_lstm_model(lstm_model, train_loader, val_loader)\n",
    "\n",
    "    # Fine-tune BERT\n",
    "    bert_model = fine_tune_bert(train_texts, train_labels, val_texts, val_labels)\n",
    "\n",
    "    # Save models\n",
    "    torch.save(lstm_model.state_dict(), \"lstm_model.pth\")\n",
    "    bert_model.save_pretrained(\"bert_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Accuracy: 0.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\王祖政\\AppData\\Local\\Temp\\ipykernel_32656\\1438812275.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(data, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Accuracy: 0.9329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9329140461215933"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def evaluate_lstm_model(model, test_data, test_labels):\n",
    "    model.eval()  # 设置为评估模式\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for i in range(len(test_data)):\n",
    "            input_data = torch.tensor(test_data[i]).unsqueeze(0).to(device)  # 增加 batch 维度\n",
    "            output = model(input_data).squeeze().cpu().numpy()\n",
    "            predictions.append(1 if output >= 0.5 else 0)  # 概率 > 0.5 则为正类\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    print(f\"LSTM Model Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_bert_model(model, test_texts, test_labels, batch_size=16):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=30, return_tensors=\"pt\")\n",
    "    test_dataset = SarcasmDataset(test_encodings['input_ids'], test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model.eval()  # 设置为评估模式\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(input_ids=inputs).logits\n",
    "            preds = torch.argmax(outputs, axis=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"BERT Model Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# 使用测试集评估 LSTM\n",
    "test_data, test_labels, _ = preprocess_data(test_texts, test_labels)\n",
    "evaluate_lstm_model(lstm_model, test_data, test_labels)\n",
    "\n",
    "# 使用测试集评估 BERT\n",
    "evaluate_bert_model(bert_model, test_texts, test_labels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training BERT Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Python\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\王祖政\\AppData\\Local\\Temp\\ipykernel_32656\\3868723138.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.2579, Val Loss: 0.2013\n",
      "Epoch 2/5, Train Loss: 0.1143, Val Loss: 0.2318\n",
      "Epoch 3/5, Train Loss: 0.0535, Val Loss: 0.2468\n",
      "Epoch 4/5, Train Loss: 0.0391, Val Loss: 0.2473\n",
      "Epoch 5/5, Train Loss: 0.0317, Val Loss: 0.3045\n",
      "Evaluating BERT Model...\n",
      "BERT Model Accuracy: 0.9210\n",
      "Confusion Matrix:\n",
      " [[1360  123]\n",
      " [ 103 1276]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1483\n",
      "           1       0.91      0.93      0.92      1379\n",
      "\n",
      "    accuracy                           0.92      2862\n",
      "   macro avg       0.92      0.92      0.92      2862\n",
      "weighted avg       0.92      0.92      0.92      2862\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    headlines = [item['headline'] for item in data]\n",
    "    labels = [item['is_sarcastic'] for item in data]\n",
    "    return headlines, labels\n",
    "\n",
    "# Dataset class for BERT\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Train BERT model\n",
    "def train_bert_model(train_texts, train_labels, val_texts, val_labels, max_length=30, batch_size=16, epochs=5):\n",
    "    # Tokenizer and encoding\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_dataset = SarcasmDataset(train_encodings, train_labels)\n",
    "    val_dataset = SarcasmDataset(val_encodings, val_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Model initialization\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Evaluate BERT model\n",
    "def evaluate_bert_model(model, tokenizer, test_texts, test_labels, max_length=30, batch_size=16):\n",
    "    # Tokenizer and encoding\n",
    "    test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    test_dataset = SarcasmDataset(test_encodings, test_labels)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs).logits\n",
    "            preds = torch.argmax(outputs, axis=1).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    print(f\"BERT Model Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    return accuracy, cm, report\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    file_path = \"Sarcasm_Headlines_Dataset.json\"\n",
    "    headlines, labels = load_data(file_path)\n",
    "    train_texts, temp_texts, train_labels, temp_labels = train_test_split(headlines, labels, test_size=0.2, random_state=42)\n",
    "    val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Train BERT model\n",
    "    print(\"Training BERT Model...\")\n",
    "    bert_model, tokenizer = train_bert_model(train_texts, train_labels, val_texts, val_labels)\n",
    "\n",
    "    # Save model\n",
    "    bert_model.save_pretrained(\"optimized_bert_model\")\n",
    "    tokenizer.save_pretrained(\"optimized_bert_model\")\n",
    "\n",
    "    # Evaluate BERT model\n",
    "    print(\"Evaluating BERT Model...\")\n",
    "    evaluate_bert_model(bert_model, tokenizer, test_texts, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Enter a sentence to analyze (type 'exit' to quit):\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Sarcastic\n",
      "Analysis: Sarcastic\n",
      "Analysis: Sarcastic\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Not Sarcastic\n",
      "Analysis: Sarcastic\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 检查设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 加载模型和分词器\n",
    "def load_model_and_tokenizer(model_path=\"optimized_bert_model\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    model.eval()  # 设置为评估模式\n",
    "    return model, tokenizer\n",
    "\n",
    "# 分析输入文本\n",
    "def analyze_text(text, model, tokenizer, max_length=30):\n",
    "    # 文本分词和编码\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, axis=1).item()  # 获取类别\n",
    "\n",
    "    # 返回分析结果\n",
    "    return \"Sarcastic\" if prediction == 1 else \"Not Sarcastic\"\n",
    "\n",
    "# 主功能\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载模型和分词器\n",
    "    model_path = \"optimized_bert_model\"  # 确保该路径存在并包含训练好的模型\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "\n",
    "    # 输入循环\n",
    "    print(\"Enter a sentence to analyze (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        user_input = input(\"Input: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        result = analyze_text(user_input, model, tokenizer)\n",
    "        print(f\"Analysis: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
